{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vamsi10010/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vamsi10010/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/vamsi10010/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "import html2text\n",
    "from urlextract import URLExtract\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_spam_data():\n",
    "    spam_root = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "    ham_url = spam_root + \"20030228_easy_ham.tar.bz2\"\n",
    "    spam_url = spam_root + \"20030228_spam.tar.bz2\"\n",
    "\n",
    "    spam_path = Path() / \"datasets\" / \"spam\"\n",
    "    spam_path.mkdir(parents=True, exist_ok=True)\n",
    "    for dir_name, tar_name, url in ((\"easy_ham\", \"ham\", ham_url),\n",
    "                                    (\"spam\", \"spam\", spam_url)):\n",
    "        if not (spam_path / dir_name).is_dir():\n",
    "            path = (spam_path / tar_name).with_suffix(\".tar.bz2\")\n",
    "            print(\"Downloading\", path)\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "            tar_bz2_file = tarfile.open(path)\n",
    "            tar_bz2_file.extractall(path=spam_path)\n",
    "            tar_bz2_file.close()\n",
    "    return [spam_path / dir_name for dir_name in (\"easy_ham\", \"spam\")]\n",
    "\n",
    "ham_dir, spam_dir = fetch_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_filenames = [f for f in sorted(ham_dir.iterdir()) if len(f.name) > 20]\n",
    "spam_filenames = [f for f in sorted(spam_dir.iterdir()) if len(f.name) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import email.policy\n",
    "\n",
    "def load_file(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_emails = [load_file(file) for file in ham_filenames]\n",
    "spam_emails = [load_file(file) for file in spam_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts email to raw text\n",
    "\n",
    "def email_to_text(email):\n",
    "    output = ''\n",
    "    for part in email.walk():\n",
    "        if part.get_content_type() == 'text/plain':\n",
    "            try:\n",
    "                output += part.get_content()\n",
    "            except:\n",
    "                output += str(part.get_payload())\n",
    "        elif part.get_content_type() == 'text/html':\n",
    "            try:\n",
    "                output += html2text.html2text(part.get_content())\n",
    "            except:\n",
    "                output += html2text.html2text(str(part.get_payload()))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test data\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(ham_emails + spam_emails, dtype=object)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_replacer(text):\n",
    "    extractor = URLExtract()\n",
    "    urls = list(set(extractor.find_urls(text)))\n",
    "    urls.sort(key=lambda url: len(url), reverse=True)\n",
    "    for url in urls:\n",
    "        text = text.replace(url, \" URL \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'\\W+', ' ', text, flags=re.M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class EmailProcessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, replace_urls=True, replace_numbers=True, remove_stopwords=True, use_headers=False):\n",
    "        self.use_headers = use_headers\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        content = []\n",
    "        for email in X:\n",
    "            sub = email['Subject']\n",
    "            text = email_to_text(email)\n",
    "            if self.replace_urls:\n",
    "                text = url_replacer(text)\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
    "                sub = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', sub)\n",
    "            \n",
    "            stemmer = PorterStemmer()\n",
    "            tokens = [stemmer.stem(s) for s in word_tokenize(remove_punctuation(text))]\n",
    "            sub_tokens = [stemmer.stem(s) for s in word_tokenize(remove_punctuation(sub))]\n",
    "            if self.remove_stopwords:\n",
    "                stop_words = set(stopwords.words('english'))\n",
    "                tokens = [i for i in tokens if i not in stop_words]\n",
    "                sub_tokens = [i for i in sub_tokens if i not in stop_words]\n",
    "            text = \" \".join(tokens)\n",
    "            sub = \" \".join(sub_tokens)\n",
    "            \n",
    "            content.append((sub + text) if self.use_headers else text)\n",
    "            \n",
    "        # output = pd.DataFrame(content, columns=['Content'])\n",
    "        return content\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess = Pipeline([\n",
    "    ('email_processor', EmailProcessor(use_headers=True)),\n",
    "    ('vectorizer', TfidfVectorizer())\n",
    "])\n",
    "\n",
    "X_train_tf = preprocess.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = MultinomialNB()\n",
    "score = cross_val_score(model, X_train_tf.toarray(), y_train, cv=10)\n",
    "\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
